{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiakavousi/ml/blob/main/NLP\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%201%20-%20Lesson%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers tensorflow pandas numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "078FZTlF8CZN",
        "outputId": "b697f52f-5da8-4a4f-e1ab-2087f00a15fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zaCMcjMQifQc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, LSTM, Dense, Dropout,\n",
        "    Bidirectional, GlobalMaxPooling1D\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalyzer:\n",
        "    def __init__(self, max_words=10000, max_len=200, embedding_dim=100):\n",
        "        self.max_words = max_words\n",
        "        self.max_len = max_len\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tokenizer = Tokenizer(num_words=max_words)\n",
        "        self.model = None\n",
        "\n",
        "    def load_and_prepare_data(self, max_samples_per_class=10000):\n",
        "        \"\"\"\n",
        "        Load and prepare Yelp dataset with balanced classes\n",
        "        \"\"\"\n",
        "        print(\"Loading Yelp dataset...\")\n",
        "        dataset = load_dataset(\"yelp_review_full\")\n",
        "        df = pd.DataFrame(dataset['train'])\n",
        "\n",
        "        # Convert 5-star ratings to 3 classes\n",
        "        def convert_to_sentiment(stars):\n",
        "            if stars <= 2:\n",
        "                return 0  # negative\n",
        "            elif stars == 3:\n",
        "                return 1  # neutral\n",
        "            else:\n",
        "                return 2  # positive\n",
        "\n",
        "        df['sentiment'] = df['label'].apply(convert_to_sentiment)\n",
        "\n",
        "        # Balance the dataset\n",
        "        balanced_data = []\n",
        "        for sentiment in [0, 1, 2]:\n",
        "            sentiment_data = df[df['sentiment'] == sentiment]\n",
        "            balanced_data.append(sentiment_data.sample(\n",
        "                n=min(max_samples_per_class, len(sentiment_data)),\n",
        "                random_state=42\n",
        "            ))\n",
        "\n",
        "        balanced_df = pd.concat(balanced_data)\n",
        "\n",
        "        # Split into train and test\n",
        "        train_df, test_df = train_test_split(\n",
        "            balanced_df,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=balanced_df['sentiment']\n",
        "        )\n",
        "\n",
        "        # Prepare text data\n",
        "        self.tokenizer.fit_on_texts(train_df['text'])\n",
        "\n",
        "        X_train = self.tokenizer.texts_to_sequences(train_df['text'])\n",
        "        X_test = self.tokenizer.texts_to_sequences(test_df['text'])\n",
        "\n",
        "        X_train = pad_sequences(X_train, maxlen=self.max_len)\n",
        "        X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
        "\n",
        "        # Convert labels to one-hot encoding\n",
        "        y_train = tf.keras.utils.to_categorical(train_df['sentiment'])\n",
        "        y_test = tf.keras.utils.to_categorical(test_df['sentiment'])\n",
        "\n",
        "        print(f\"Training samples: {len(X_train)}\")\n",
        "        print(f\"Testing samples: {len(X_test)}\")\n",
        "\n",
        "        return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build model architecture\n",
        "        \"\"\"\n",
        "        input_layer = Input(shape=(self.max_len,))\n",
        "\n",
        "        embedding = Embedding(\n",
        "            self.max_words,\n",
        "            self.embedding_dim,\n",
        "            input_length=self.max_len\n",
        "        )(input_layer)\n",
        "\n",
        "        # Bidirectional LSTM layers\n",
        "        lstm_1 = Bidirectional(LSTM(64, return_sequences=True))(embedding)\n",
        "        lstm_2 = Bidirectional(LSTM(32, return_sequences=True))(lstm_1)\n",
        "\n",
        "        # Global pooling\n",
        "        pooled = GlobalMaxPooling1D()(lstm_2)\n",
        "\n",
        "        # Dense layers with dropout\n",
        "        dense_1 = Dense(64, activation='relu')(pooled)\n",
        "        dropout_1 = Dropout(0.3)(dense_1)\n",
        "\n",
        "        dense_2 = Dense(32, activation='relu')(dropout_1)\n",
        "        dropout_2 = Dropout(0.2)(dense_2)\n",
        "\n",
        "        # Output layer (3 classes)\n",
        "        output_layer = Dense(3, activation='softmax')(dropout_2)\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def train(self, X_train, y_train, validation_data=None, epochs=10):\n",
        "        \"\"\"\n",
        "        Train the model\n",
        "        \"\"\"\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            epochs=epochs,\n",
        "            validation_data=validation_data,\n",
        "            callbacks=[early_stopping],\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, texts):\n",
        "        \"\"\"\n",
        "        Make predictions on new texts\n",
        "        \"\"\"\n",
        "        # Prepare input data\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = self.model.predict(X)\n",
        "\n",
        "        # Convert predictions to labels\n",
        "        labels = ['negative', 'neutral', 'positive']\n",
        "        results = []\n",
        "\n",
        "        for pred in predictions:\n",
        "            label_idx = np.argmax(pred)\n",
        "            confidence = float(pred[label_idx])\n",
        "\n",
        "            # If confidence is low or scores are close, consider neutral\n",
        "            if confidence < 0.5 or (\n",
        "                abs(pred[0] - pred[2]) < 0.2  # Difference between pos and neg\n",
        "            ):\n",
        "                label_idx = 1  # neutral\n",
        "                confidence = float(pred[1])\n",
        "\n",
        "            results.append({\n",
        "                'sentiment': labels[label_idx],\n",
        "                'confidence': confidence,\n",
        "                'scores': {\n",
        "                    'negative': float(pred[0]),\n",
        "                    'neutral': float(pred[1]),\n",
        "                    'positive': float(pred[2])\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "hlVsAAmb1xV2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_sentiment_analyzer(analyzer):\n",
        "    \"\"\"\n",
        "    Test the sentiment analyzer with various types of reviews\n",
        "    \"\"\"\n",
        "    test_cases = {\n",
        "        \"Positive Reviews\": [\n",
        "            \"This is absolutely amazing! The best experience I've ever had!\",\n",
        "            \"Outstanding service and incredible quality. Highly recommend to everyone!\",\n",
        "            \"Exceeded all my expectations. Perfect in every way possible!\"\n",
        "        ],\n",
        "\n",
        "        \"Negative Reviews\": [\n",
        "            \"Terrible experience. Worst service I've ever encountered.\",\n",
        "            \"Complete waste of money. Extremely disappointing quality.\",\n",
        "            \"Absolutely horrible. Would never recommend this to anyone.\"\n",
        "        ],\n",
        "\n",
        "        \"Neutral Reviews\": [\n",
        "            \"It's okay, nothing special but nothing terrible either.\",\n",
        "            \"Average experience. Does the job but wouldn't go out of my way for it.\",\n",
        "            \"Decent enough. Has both good and bad points, mostly mediocre.\"\n",
        "        ],\n",
        "\n",
        "        \"Sarcastic Reviews\": [\n",
        "            \"Oh yeah, 'AMAZING' service... if you enjoy waiting for hours! *slow clap*\",\n",
        "            \"Wow, what a 'fantastic' experience... if you like throwing money away!\",\n",
        "            \"Just 'perfect'... if you're a fan of complete disappointment! 👏\"\n",
        "        ],\n",
        "\n",
        "        \"Reviews with Negation\": [\n",
        "            \"Not as bad as I expected, actually quite good.\",\n",
        "            \"This isn't terrible at all, rather enjoyable.\",\n",
        "            \"The service wasn't great, but not horrible either.\"\n",
        "        ],\n",
        "\n",
        "        \"Reviews with Multipolarity\": [\n",
        "            \"Great food but terrible service. Can't decide if I'll return.\",\n",
        "            \"Amazing quality products, however extremely overpriced. Mixed feelings overall.\",\n",
        "            \"While the interface is beautiful, the functionality is poor. Torn about this one.\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    print(\"Testing model with different types of reviews...\")\n",
        "\n",
        "    for category, reviews in test_cases.items():\n",
        "        print(f\"\\n{'-'*20} {category} {'-'*20}\")\n",
        "\n",
        "        results = analyzer.predict(reviews)\n",
        "\n",
        "        for review, result in zip(reviews, results):\n",
        "            print(f\"\\nText: {review}\")\n",
        "            print(f\"Predicted Sentiment: {result['sentiment']}\")\n",
        "            print(f\"Confidence: {result['confidence']:.2f}\")\n",
        "            print(\"Sentiment Scores:\")\n",
        "            for sentiment, score in result['scores'].items():\n",
        "                print(f\"  {sentiment}: {score:.3f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    return test_cases\n",
        "\n",
        "# Example usage:\n",
        "def main():\n",
        "    # Initialize and train your model first\n",
        "    analyzer = SentimentAnalyzer()\n",
        "\n",
        "    # Load and prepare data\n",
        "    (X_train, y_train), (X_test, y_test) = analyzer.load_and_prepare_data()\n",
        "\n",
        "    # Build and train model\n",
        "    model = analyzer.build_model()\n",
        "    analyzer.train(X_train, y_train, validation_data=(X_test, y_test), epochs=5)\n",
        "\n",
        "    # Run comprehensive tests\n",
        "    test_cases = test_sentiment_analyzer(analyzer)\n",
        "\n",
        "    # Additional analysis\n",
        "    analyze_results(test_cases, analyzer)\n",
        "\n",
        "def analyze_results(test_cases, analyzer):\n",
        "    \"\"\"\n",
        "    Analyze model performance on different types of reviews\n",
        "    \"\"\"\n",
        "    print(\"\\nPerformance Analysis:\")\n",
        "\n",
        "    for category, reviews in test_cases.items():\n",
        "        results = analyzer.predict(reviews)\n",
        "\n",
        "        # Calculate average confidence\n",
        "        avg_confidence = np.mean([r['confidence'] for r in results])\n",
        "\n",
        "        # Calculate sentiment distribution\n",
        "        sentiment_dist = {}\n",
        "        for result in results:\n",
        "            sentiment = result['sentiment']\n",
        "            sentiment_dist[sentiment] = sentiment_dist.get(sentiment, 0) + 1\n",
        "\n",
        "        print(f\"\\n{category}:\")\n",
        "        print(f\"Average Confidence: {avg_confidence:.2f}\")\n",
        "        print(\"Sentiment Distribution:\", sentiment_dist)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72-KfYDP13cp",
        "outputId": "dd6bcb6c-05f4-40c8-fd09-35cc6dd0136f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Yelp dataset...\n",
            "Training samples: 24000\n",
            "Testing samples: 6000\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.4956 - loss: 0.9537 - val_accuracy: 0.6850 - val_loss: 0.6923\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.7241 - loss: 0.6360 - val_accuracy: 0.6903 - val_loss: 0.7049\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.7856 - loss: 0.5183 - val_accuracy: 0.6893 - val_loss: 0.7438\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.8360 - loss: 0.4198 - val_accuracy: 0.6725 - val_loss: 0.7561\n",
            "Testing model with different types of reviews...\n",
            "\n",
            "-------------------- Positive Reviews --------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step\n",
            "\n",
            "Text: This is absolutely amazing! The best experience I've ever had!\n",
            "Predicted Sentiment: positive\n",
            "Confidence: 0.96\n",
            "Sentiment Scores:\n",
            "  negative: 0.001\n",
            "  neutral: 0.043\n",
            "  positive: 0.956\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: Outstanding service and incredible quality. Highly recommend to everyone!\n",
            "Predicted Sentiment: positive\n",
            "Confidence: 0.96\n",
            "Sentiment Scores:\n",
            "  negative: 0.000\n",
            "  neutral: 0.041\n",
            "  positive: 0.958\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: Exceeded all my expectations. Perfect in every way possible!\n",
            "Predicted Sentiment: positive\n",
            "Confidence: 0.89\n",
            "Sentiment Scores:\n",
            "  negative: 0.004\n",
            "  neutral: 0.105\n",
            "  positive: 0.891\n",
            "--------------------------------------------------\n",
            "\n",
            "-------------------- Negative Reviews --------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\n",
            "Text: Terrible experience. Worst service I've ever encountered.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.99\n",
            "Sentiment Scores:\n",
            "  negative: 0.993\n",
            "  neutral: 0.005\n",
            "  positive: 0.002\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: Complete waste of money. Extremely disappointing quality.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.97\n",
            "Sentiment Scores:\n",
            "  negative: 0.971\n",
            "  neutral: 0.020\n",
            "  positive: 0.009\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: Absolutely horrible. Would never recommend this to anyone.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.90\n",
            "Sentiment Scores:\n",
            "  negative: 0.905\n",
            "  neutral: 0.054\n",
            "  positive: 0.041\n",
            "--------------------------------------------------\n",
            "\n",
            "-------------------- Neutral Reviews --------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\n",
            "Text: It's okay, nothing special but nothing terrible either.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.99\n",
            "Sentiment Scores:\n",
            "  negative: 0.995\n",
            "  neutral: 0.004\n",
            "  positive: 0.001\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: Average experience. Does the job but wouldn't go out of my way for it.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.95\n",
            "Sentiment Scores:\n",
            "  negative: 0.955\n",
            "  neutral: 0.031\n",
            "  positive: 0.014\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: Decent enough. Has both good and bad points, mostly mediocre.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.98\n",
            "Sentiment Scores:\n",
            "  negative: 0.984\n",
            "  neutral: 0.012\n",
            "  positive: 0.004\n",
            "--------------------------------------------------\n",
            "\n",
            "-------------------- Sarcastic Reviews --------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\n",
            "Text: Oh yeah, 'AMAZING' service... if you enjoy waiting for hours! *slow clap*\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.85\n",
            "Sentiment Scores:\n",
            "  negative: 0.851\n",
            "  neutral: 0.105\n",
            "  positive: 0.044\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: Wow, what a 'fantastic' experience... if you like throwing money away!\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.67\n",
            "Sentiment Scores:\n",
            "  negative: 0.670\n",
            "  neutral: 0.184\n",
            "  positive: 0.146\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: Just 'perfect'... if you're a fan of complete disappointment! 👏\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.85\n",
            "Sentiment Scores:\n",
            "  negative: 0.848\n",
            "  neutral: 0.103\n",
            "  positive: 0.049\n",
            "--------------------------------------------------\n",
            "\n",
            "-------------------- Reviews with Negation --------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\n",
            "Text: Not as bad as I expected, actually quite good.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.68\n",
            "Sentiment Scores:\n",
            "  negative: 0.680\n",
            "  neutral: 0.204\n",
            "  positive: 0.116\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: This isn't terrible at all, rather enjoyable.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.98\n",
            "Sentiment Scores:\n",
            "  negative: 0.982\n",
            "  neutral: 0.013\n",
            "  positive: 0.005\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: The service wasn't great, but not horrible either.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.97\n",
            "Sentiment Scores:\n",
            "  negative: 0.971\n",
            "  neutral: 0.023\n",
            "  positive: 0.007\n",
            "--------------------------------------------------\n",
            "\n",
            "-------------------- Reviews with Multipolarity --------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\n",
            "Text: Great food but terrible service. Can't decide if I'll return.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.98\n",
            "Sentiment Scores:\n",
            "  negative: 0.985\n",
            "  neutral: 0.012\n",
            "  positive: 0.003\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: Amazing quality products, however extremely overpriced. Mixed feelings overall.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.66\n",
            "Sentiment Scores:\n",
            "  negative: 0.655\n",
            "  neutral: 0.193\n",
            "  positive: 0.151\n",
            "--------------------------------------------------\n",
            "\n",
            "Text: While the interface is beautiful, the functionality is poor. Torn about this one.\n",
            "Predicted Sentiment: negative\n",
            "Confidence: 0.92\n",
            "Sentiment Scores:\n",
            "  negative: 0.918\n",
            "  neutral: 0.052\n",
            "  positive: 0.030\n",
            "--------------------------------------------------\n",
            "\n",
            "Performance Analysis:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\n",
            "Positive Reviews:\n",
            "Average Confidence: 0.94\n",
            "Sentiment Distribution: {'positive': 3}\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\n",
            "Negative Reviews:\n",
            "Average Confidence: 0.96\n",
            "Sentiment Distribution: {'negative': 3}\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\n",
            "Neutral Reviews:\n",
            "Average Confidence: 0.98\n",
            "Sentiment Distribution: {'negative': 3}\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\n",
            "Sarcastic Reviews:\n",
            "Average Confidence: 0.79\n",
            "Sentiment Distribution: {'negative': 3}\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\n",
            "Reviews with Negation:\n",
            "Average Confidence: 0.88\n",
            "Sentiment Distribution: {'negative': 3}\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\n",
            "Reviews with Multipolarity:\n",
            "Average Confidence: 0.85\n",
            "Sentiment Distribution: {'negative': 3}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Course 3 - Week 1 - Lesson 1.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}